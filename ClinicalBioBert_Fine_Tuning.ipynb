{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification\n",
    ")\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import Trainer, TrainingArguments, AutoTokenizer, AutoModelForSequenceClassification\n",
    "import os\n",
    "from torch.utils.data import TensorDataset\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "from torch.utils.data import DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add path to your root dir (Training Folder)\n",
    "\n",
    "folder_path= 'E:\\\\Case Comp\\\\NEST\\\\Training\\\\'\n",
    "epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 257577 entries, 0 to 257576\n",
      "Data columns (total 32 columns):\n",
      " #   Column                      Non-Null Count   Dtype  \n",
      "---  ------                      --------------   -----  \n",
      " 0   Unnamed: 0.1                257577 non-null  int64  \n",
      " 1   Unnamed: 0                  257577 non-null  int64  \n",
      " 2   NCT Number                  257577 non-null  object \n",
      " 3   Study Title                 257577 non-null  object \n",
      " 4   Study URL                   257577 non-null  object \n",
      " 5   Acronym                     63991 non-null   object \n",
      " 6   Study Status                257577 non-null  object \n",
      " 7   Brief Summary               257577 non-null  object \n",
      " 8   Study Results               257577 non-null  object \n",
      " 9   Conditions                  257577 non-null  object \n",
      " 10  Interventions               234064 non-null  object \n",
      " 11  Primary Outcome Measures    247086 non-null  object \n",
      " 12  Secondary Outcome Measures  185779 non-null  object \n",
      " 13  Other Outcome Measures      18272 non-null   object \n",
      " 14  Sponsor                     257577 non-null  object \n",
      " 15  Collaborators               83679 non-null   object \n",
      " 16  Sex                         257317 non-null  object \n",
      " 17  Age                         257577 non-null  object \n",
      " 18  Phases                      112765 non-null  object \n",
      " 19  Enrollment                  254205 non-null  float64\n",
      " 20  Funder Type                 257577 non-null  object \n",
      " 21  Study Type                  257577 non-null  object \n",
      " 22  Study Design                257577 non-null  object \n",
      " 23  Other IDs                   257556 non-null  object \n",
      " 24  Start Date                  255245 non-null  object \n",
      " 25  Primary Completion Date     244673 non-null  object \n",
      " 26  Completion Date             249686 non-null  object \n",
      " 27  First Posted                257577 non-null  object \n",
      " 28  Results First Posted        51581 non-null   object \n",
      " 29  Last Update Posted          257577 non-null  object \n",
      " 30  Locations                   233966 non-null  object \n",
      " 31  Study Documents             25155 non-null   object \n",
      "dtypes: float64(1), int64(2), object(29)\n",
      "memory usage: 62.9+ MB\n",
      "Initial Dataset Info:\n",
      " None\n",
      "\n",
      "Sample Data:\n",
      "    Unnamed: 0.1  Unnamed: 0   NCT Number  \\\n",
      "0         75209      118014  NCT00559130   \n",
      "1          6356        9987  NCT00937664   \n",
      "2        143427      226012  NCT00441597   \n",
      "3        138885      218952  NCT03296228   \n",
      "4          9769       15382  NCT00421603   \n",
      "\n",
      "                                         Study Title  \\\n",
      "0  Efficacy Study of CytoSorb Hemoperfusion Devic...   \n",
      "1  Safety and Tolerability Study of AZD7762 in Co...   \n",
      "2  Does Atorvastatin Reduce Ischemia-Reperfusion ...   \n",
      "3  Comparison of Dynamic Radiographs in Determini...   \n",
      "4  A Placebo-Controlled Study of Mixed Amphetamin...   \n",
      "\n",
      "                                      Study URL Acronym Study Status  \\\n",
      "0  https://clinicaltrials.gov/study/NCT00559130     NaN    COMPLETED   \n",
      "1  https://clinicaltrials.gov/study/NCT00937664     NaN   TERMINATED   \n",
      "2  https://clinicaltrials.gov/study/NCT00441597     NaN    COMPLETED   \n",
      "3  https://clinicaltrials.gov/study/NCT03296228     NaN    COMPLETED   \n",
      "4  https://clinicaltrials.gov/study/NCT00421603    TACT    COMPLETED   \n",
      "\n",
      "                                       Brief Summary Study Results  \\\n",
      "0  The hypothesis of this study is use of CytoSor...            NO   \n",
      "1  The primary purpose of this study is to find o...            NO   \n",
      "2  To study the impact of 3 day exposure to atorv...            NO   \n",
      "3  The purpose of this study is to identify the f...            NO   \n",
      "4  The proposed protocol is a double-blind, place...           YES   \n",
      "\n",
      "                                          Conditions  ...  \\\n",
      "0  Acute Respiratory Distress Syndrome|Acute Lung...  ...   \n",
      "1    Cancer|Solid Tumors|Advanced Solid Malignancies  ...   \n",
      "2  Ischemia Reperfusion Injury|Cardiovascular Dis...  ...   \n",
      "3                    Adolescent Idiopathic Scoliosis  ...   \n",
      "4                                 Cocaine Dependence  ...   \n",
      "\n",
      "                                        Study Design          Other IDs  \\\n",
      "0  Allocation: RANDOMIZED|Intervention Model: PAR...            2007-01   \n",
      "1  Allocation: NON_RANDOMIZED|Intervention Model:...        D1040C00008   \n",
      "2  Allocation: RANDOMIZED|Intervention Model: CRO...            atorv01   \n",
      "3          Observational Model: |Time Perspective: p          UW 16-208   \n",
      "4  Allocation: RANDOMIZED|Intervention Model: PAR...  #5368|R01DA022217   \n",
      "\n",
      "   Start Date Primary Completion Date Completion Date First Posted  \\\n",
      "0     2007-11                 2011-04         2011-06   2007-11-16   \n",
      "1     2009-07                 2011-02         2011-02   2009-07-13   \n",
      "2     2007-02                 2009-02         2009-03   2007-03-01   \n",
      "3  2016-05-01              2018-05-01      2018-12-31   2017-09-28   \n",
      "4     2007-02                 2010-05         2010-05   2007-01-12   \n",
      "\n",
      "  Results First Posted Last Update Posted  \\\n",
      "0                  NaN         2011-06-07   \n",
      "1                  NaN         2011-02-07   \n",
      "2                  NaN         2009-03-17   \n",
      "3                  NaN         2020-05-06   \n",
      "4           2013-02-28         2019-04-24   \n",
      "\n",
      "                                           Locations  Study Documents  \n",
      "0  Aachen, Germany|Berlin, Germany|Bonn, Germany|...              NaN  \n",
      "1           Research Site, Minami-ku, Fukuoka, Japan              NaN  \n",
      "2  Radboud University Nijmegen Medical Centre, Ni...              NaN  \n",
      "3  Duchess of Kent Children's Hospital, Hong Kong...              NaN  \n",
      "4    STARS, New York, New York, 10032, United States              NaN  \n",
      "\n",
      "[5 rows x 32 columns]\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Load Dataset\n",
    "def load_data(file_path):\n",
    "    \"\"\"Loads the dataset and provides an initial overview.\"\"\"\n",
    "    data = pd.read_csv(file_path)\n",
    "    print(\"Initial Dataset Info:\\n\", data.info())\n",
    "    print(\"\\nSample Data:\\n\", data.head())\n",
    "    return data\n",
    "\n",
    "\n",
    "df = load_data(folder_path + \"usecase_3_.csv\")\n",
    "test_df = pd.read_excel(folder_path + \"TESTWITHSTUDY.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.head(500)\n",
    "test_df = test_df.head(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Unnamed: 0.1',\n",
       " 'Unnamed: 0',\n",
       " 'NCT Number',\n",
       " 'Study Title',\n",
       " 'Study URL',\n",
       " 'Acronym',\n",
       " 'Study Status',\n",
       " 'Brief Summary',\n",
       " 'Study Results',\n",
       " 'Conditions',\n",
       " 'Interventions',\n",
       " 'Primary Outcome Measures',\n",
       " 'Secondary Outcome Measures',\n",
       " 'Other Outcome Measures',\n",
       " 'Sponsor',\n",
       " 'Collaborators',\n",
       " 'Sex',\n",
       " 'Age',\n",
       " 'Phases',\n",
       " 'Enrollment',\n",
       " 'Funder Type',\n",
       " 'Study Type',\n",
       " 'Study Design',\n",
       " 'Other IDs',\n",
       " 'Start Date',\n",
       " 'Primary Completion Date',\n",
       " 'Completion Date',\n",
       " 'First Posted',\n",
       " 'Results First Posted',\n",
       " 'Last Update Posted',\n",
       " 'Locations',\n",
       " 'Study Documents']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 31)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = test_df.dropna(how='all')\n",
    "test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all text attributes into a single column 'Unstructured'\n",
    "df[\"Outcomes\"] = df[\n",
    "    [\"Primary Outcome Measures\", \"Conditions\", \"Secondary Outcome Measures\", \"Other Outcome Measures\"]\n",
    "].astype(str).agg(\" [SEP] \".join, axis=1)\n",
    "\n",
    "test_df[\"Outcomes\"] = test_df[\n",
    "    [\"Primary Outcome Measures\", \"Conditions\", \"Secondary Outcome Measures\", \"Other Outcome Measures\"]\n",
    "].astype(str).agg(\" [SEP] \".join, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         Brief Summary  \\\n",
      "0    The hypothesis of this study is use of CytoSor...   \n",
      "1    The primary purpose of this study is to find o...   \n",
      "2    To study the impact of 3 day exposure to atorv...   \n",
      "3    The purpose of this study is to identify the f...   \n",
      "4    The proposed protocol is a double-blind, place...   \n",
      "..                                                 ...   \n",
      "495  To analyze heterogeneity in ADHD experts in la...   \n",
      "496  Periodontitis patients, 40 cigarette smokers a...   \n",
      "497  This was a double-blinded randomized controlle...   \n",
      "498  This clinical trial is Phase II trial for eval...   \n",
      "499  This is a single-center, open-label phase I cl...   \n",
      "\n",
      "                                           Study Title  \\\n",
      "0    Efficacy Study of CytoSorb Hemoperfusion Devic...   \n",
      "1    Safety and Tolerability Study of AZD7762 in Co...   \n",
      "2    Does Atorvastatin Reduce Ischemia-Reperfusion ...   \n",
      "3    Comparison of Dynamic Radiographs in Determini...   \n",
      "4    A Placebo-Controlled Study of Mixed Amphetamin...   \n",
      "..                                                 ...   \n",
      "495  Temperament Dimensions and Awakening Salivary ...   \n",
      "496  The Impact of Cigarette Smoking on Periodontal...   \n",
      "497  Short-term Effects of Perindopril-amlodipine V...   \n",
      "498  Efficacy Study of Genexol-PM and Cisplatin in ...   \n",
      "499  A Study to Investigate the Drug-drug Interacti...   \n",
      "\n",
      "                                            Conditions  \\\n",
      "0    Acute Respiratory Distress Syndrome|Acute Lung...   \n",
      "1      Cancer|Solid Tumors|Advanced Solid Malignancies   \n",
      "2    Ischemia Reperfusion Injury|Cardiovascular Dis...   \n",
      "3                      Adolescent Idiopathic Scoliosis   \n",
      "4                                   Cocaine Dependence   \n",
      "..                                                 ...   \n",
      "495           Attention Deficit Hyperactivity Disorder   \n",
      "496                    Periodontitis|Cigarette Smoking   \n",
      "497             Diabetes Mellitus, Type 2|Hypertension   \n",
      "498  Locally Advanced Head and Neck Squamous Cell C...   \n",
      "499                                   Healthy Subjects   \n",
      "\n",
      "                                              Outcomes   NCT Number  \\\n",
      "0    Relative IL-6 levels as a percent (%) of basel...  NCT00559130   \n",
      "1    Assessment of adverse events (based on CTCAE v...  NCT00937664   \n",
      "2    Annexin A 5 targeting in the non dominant then...  NCT00441597   \n",
      "3    Investigate the flexibility equivalence of dif...  NCT03296228   \n",
      "4    Three Weeks of Continuous Cocaine Abstinence a...  NCT00421603   \n",
      "..                                                 ...          ...   \n",
      "495  To examine the association of ADHD with temper...  NCT04326543   \n",
      "496  PD ≥5 mm with BoP, Corrected for clustering wi...  NCT05120206   \n",
      "497  24-hour blood pressure, Change in average 24-h...  NCT03747978   \n",
      "498  response rate, RECIST, every 2 cycles, 6wk lat...  NCT01689194   \n",
      "499  Maximum concentration (Cmax)of Midazolam and i...  NCT05070195   \n",
      "\n",
      "    Study Status  \n",
      "0      COMPLETED  \n",
      "1     TERMINATED  \n",
      "2      COMPLETED  \n",
      "3      COMPLETED  \n",
      "4      COMPLETED  \n",
      "..           ...  \n",
      "495    COMPLETED  \n",
      "496    COMPLETED  \n",
      "497    COMPLETED  \n",
      "498    COMPLETED  \n",
      "499    COMPLETED  \n",
      "\n",
      "[500 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "# Drop the original text columns\n",
    "df = df[[\"Brief Summary\", \"Study Title\", \"Conditions\", \"Outcomes\", \"NCT Number\", \"Study Status\"]]\n",
    "test_df = test_df[[\"Brief Summary\", \"Study Title\", \"Conditions\", \"Outcomes\", \"NCT Number\", \"Study Status\"]]\n",
    "# Display updated DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated Training DataFrame (df):\n",
      "                                       Brief Summary  \\\n",
      "0  The hypothesis of this study is use of CytoSor...   \n",
      "1  The primary purpose of this study is to find o...   \n",
      "2  To study the impact of 3 day exposure to atorv...   \n",
      "3  The purpose of this study is to identify the f...   \n",
      "4  The proposed protocol is a double-blind, place...   \n",
      "\n",
      "                                         Study Title  \\\n",
      "0  Efficacy Study of CytoSorb Hemoperfusion Devic...   \n",
      "1  Safety and Tolerability Study of AZD7762 in Co...   \n",
      "2  Does Atorvastatin Reduce Ischemia-Reperfusion ...   \n",
      "3  Comparison of Dynamic Radiographs in Determini...   \n",
      "4  A Placebo-Controlled Study of Mixed Amphetamin...   \n",
      "\n",
      "                                          Conditions  \\\n",
      "0  Acute Respiratory Distress Syndrome|Acute Lung...   \n",
      "1    Cancer|Solid Tumors|Advanced Solid Malignancies   \n",
      "2  Ischemia Reperfusion Injury|Cardiovascular Dis...   \n",
      "3                    Adolescent Idiopathic Scoliosis   \n",
      "4                                 Cocaine Dependence   \n",
      "\n",
      "                                            Outcomes   NCT Number Study Status  \n",
      "0  Relative IL-6 levels as a percent (%) of basel...  NCT00559130    COMPLETED  \n",
      "1  Assessment of adverse events (based on CTCAE v...  NCT00937664   TERMINATED  \n",
      "2  Annexin A 5 targeting in the non dominant then...  NCT00441597    COMPLETED  \n",
      "3  Investigate the flexibility equivalence of dif...  NCT03296228    COMPLETED  \n",
      "4  Three Weeks of Continuous Cocaine Abstinence a...  NCT00421603    COMPLETED  \n",
      "\n",
      "Updated Test DataFrame (test_df):\n",
      "                                       Brief Summary  \\\n",
      "0  This study is a post-market clinical follow-up...   \n",
      "1  The aim of the study is to evaluate the effica...   \n",
      "2  This phase I trial studies how well durvalumab...   \n",
      "3  The incorporation of novel targeted therapies ...   \n",
      "4  The use of nonsurgical periodontal treatment, ...   \n",
      "\n",
      "                                         Study Title  \\\n",
      "0        Patient Outcomes Using an Expandable Spacer   \n",
      "1  the Effect of Isosorbide Mononitrate in Reduci...   \n",
      "2  Durvalumab With or Without Tremelimumab in Tre...   \n",
      "3  Radiation and Cetuximab Plus Intratumoral EGFR...   \n",
      "4      Laser Biostimulation in Periodontal Treatment   \n",
      "\n",
      "                                          Conditions  \\\n",
      "0                          Degenerative Disc Disease   \n",
      "1                                 IUD Insertion Pain   \n",
      "2  Stage II Oropharyngeal Squamous Cell Carcinoma...   \n",
      "3       Squamous Cell Carcinoma|Head and Neck Cancer   \n",
      "4      Periodontal Inflammation|Periodontal Diseases   \n",
      "\n",
      "                                            Outcomes   NCT Number Study Status  \n",
      "0  Change in Radiographic Analysis, Global and Se...  NCT03162666    WITHDRAWN  \n",
      "1  pain during IUD insertion, intensity of patien...  NCT04312048    COMPLETED  \n",
      "2  Change of CD8+ tumor infiltrating lymphocytes,...  NCT03144778    COMPLETED  \n",
      "3  Toxicity Rate, This is a 2-stage clinical tria...  NCT01592721    COMPLETED  \n",
      "4  IL-1β level in GCF, IL-1β is a cytokine presen...  NCT04253613    COMPLETED  \n",
      "\n",
      "Missing Values in Training DataFrame (df):\n",
      "Brief Summary    0\n",
      "Study Title      0\n",
      "Conditions       0\n",
      "Outcomes         0\n",
      "NCT Number       0\n",
      "Study Status     0\n",
      "dtype: int64\n",
      "No missing values found in critical columns ['NCT Number', 'Study Status'] of Training DataFrame (df).\n",
      "Filled missing values in non-critical columns with 'Unknown' in Training DataFrame (df).\n",
      "\n",
      "Missing Values in Test DataFrame (test_df):\n",
      "Brief Summary    0\n",
      "Study Title      0\n",
      "Conditions       0\n",
      "Outcomes         0\n",
      "NCT Number       0\n",
      "Study Status     0\n",
      "dtype: int64\n",
      "No missing values found in critical columns ['NCT Number', 'Study Status'] of Test DataFrame (test_df).\n",
      "Filled missing values in non-critical columns with 'Unknown' in Test DataFrame (test_df).\n",
      "\n",
      "Final Missing Values in Training DataFrame (df):\n",
      "Brief Summary    0\n",
      "Study Title      0\n",
      "Conditions       0\n",
      "Outcomes         0\n",
      "NCT Number       0\n",
      "Study Status     0\n",
      "dtype: int64\n",
      "\n",
      "Final Missing Values in Test DataFrame (test_df):\n",
      "Brief Summary    0\n",
      "Study Title      0\n",
      "Conditions       0\n",
      "Outcomes         0\n",
      "NCT Number       0\n",
      "Study Status     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 1. Select Specific Columns\n",
    "selected_columns = [\"Brief Summary\", \"Study Title\", \"Conditions\", \"Outcomes\", \"NCT Number\", \"Study Status\"]\n",
    "\n",
    "df = df[selected_columns]\n",
    "test_df = test_df[selected_columns]\n",
    "\n",
    "# 2. Display the Updated DataFrame\n",
    "print(\"Updated Training DataFrame (df):\")\n",
    "print(df.head())  # Display the first few rows for brevity\n",
    "\n",
    "print(\"\\nUpdated Test DataFrame (test_df):\")\n",
    "print(test_df.head())\n",
    "\n",
    "# 3. Verify and Handle Missing Data\n",
    "\n",
    "def verify_and_handle_missing_data(dataframe, dataframe_name=\"DataFrame\"):\n",
    "    # Check for missing values\n",
    "    missing_values = dataframe.isnull().sum()\n",
    "    print(f\"\\nMissing Values in {dataframe_name}:\")\n",
    "    print(missing_values)\n",
    "    \n",
    "    # Columns that should not have missing values\n",
    "    critical_columns = [\"NCT Number\", \"Study Status\"]\n",
    "    \n",
    "    # Check if critical columns have missing values\n",
    "    missing_in_critical = dataframe[critical_columns].isnull().sum()\n",
    "    if missing_in_critical.any():\n",
    "        missing_cols = missing_in_critical[missing_in_critical > 0].index.tolist()\n",
    "        raise ValueError(f\"Missing values found in critical columns {missing_cols} of {dataframe_name}.\")\n",
    "    else:\n",
    "        print(f\"No missing values found in critical columns {critical_columns} of {dataframe_name}.\")\n",
    "    \n",
    "    # Fill missing values in other columns with \"Unknown\"\n",
    "    non_critical_columns = [col for col in dataframe.columns if col not in critical_columns]\n",
    "    dataframe[non_critical_columns] = dataframe[non_critical_columns].fillna(\"Unknown\")\n",
    "    print(f\"Filled missing values in non-critical columns with 'Unknown' in {dataframe_name}.\")\n",
    "\n",
    "# Apply the verification and handling to both df and test_df\n",
    "try:\n",
    "    verify_and_handle_missing_data(df, \"Training DataFrame (df)\")\n",
    "    verify_and_handle_missing_data(test_df, \"Test DataFrame (test_df)\")\n",
    "except ValueError as e:\n",
    "    print(e)\n",
    "    # Depending on your use case, you might want to handle the error differently\n",
    "    # For example, you could remove rows with missing critical data:\n",
    "    # df.dropna(subset=critical_columns, inplace=True)\n",
    "    # Or take other appropriate actions\n",
    "\n",
    "# Optional: Verify that there are no missing values left\n",
    "print(\"\\nFinal Missing Values in Training DataFrame (df):\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "print(\"\\nFinal Missing Values in Test DataFrame (test_df):\")\n",
    "print(test_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame head:\n",
      "                                       Brief Summary  \\\n",
      "0  The hypothesis of this study is use of CytoSor...   \n",
      "1  The primary purpose of this study is to find o...   \n",
      "2  To study the impact of 3 day exposure to atorv...   \n",
      "3  The purpose of this study is to identify the f...   \n",
      "4  The proposed protocol is a double-blind, place...   \n",
      "\n",
      "                                         Study Title  \\\n",
      "0  Efficacy Study of CytoSorb Hemoperfusion Devic...   \n",
      "1  Safety and Tolerability Study of AZD7762 in Co...   \n",
      "2  Does Atorvastatin Reduce Ischemia-Reperfusion ...   \n",
      "3  Comparison of Dynamic Radiographs in Determini...   \n",
      "4  A Placebo-Controlled Study of Mixed Amphetamin...   \n",
      "\n",
      "                                          Conditions  \\\n",
      "0  Acute Respiratory Distress Syndrome|Acute Lung...   \n",
      "1    Cancer|Solid Tumors|Advanced Solid Malignancies   \n",
      "2  Ischemia Reperfusion Injury|Cardiovascular Dis...   \n",
      "3                    Adolescent Idiopathic Scoliosis   \n",
      "4                                 Cocaine Dependence   \n",
      "\n",
      "                                            Outcomes   NCT Number Study Status  \n",
      "0  Relative IL-6 levels as a percent (%) of basel...  NCT00559130    COMPLETED  \n",
      "1  Assessment of adverse events (based on CTCAE v...  NCT00937664   TERMINATED  \n",
      "2  Annexin A 5 targeting in the non dominant then...  NCT00441597    COMPLETED  \n",
      "3  Investigate the flexibility equivalence of dif...  NCT03296228    COMPLETED  \n",
      "4  Three Weeks of Continuous Cocaine Abstinence a...  NCT00421603    COMPLETED  \n",
      "Total samples in dataset: 500\n"
     ]
    }
   ],
   "source": [
    "print(\"DataFrame head:\")\n",
    "print(df.head())\n",
    "\n",
    "# Optional: Check how many rows we have\n",
    "print(f\"Total samples in dataset: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Brief Summary</th>\n",
       "      <th>Study Title</th>\n",
       "      <th>Conditions</th>\n",
       "      <th>Outcomes</th>\n",
       "      <th>NCT Number</th>\n",
       "      <th>Study Status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This study is a post-market clinical follow-up...</td>\n",
       "      <td>Patient Outcomes Using an Expandable Spacer</td>\n",
       "      <td>Degenerative Disc Disease</td>\n",
       "      <td>Change in Radiographic Analysis, Global and Se...</td>\n",
       "      <td>NCT03162666</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The aim of the study is to evaluate the effica...</td>\n",
       "      <td>the Effect of Isosorbide Mononitrate in Reduci...</td>\n",
       "      <td>IUD Insertion Pain</td>\n",
       "      <td>pain during IUD insertion, intensity of patien...</td>\n",
       "      <td>NCT04312048</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This phase I trial studies how well durvalumab...</td>\n",
       "      <td>Durvalumab With or Without Tremelimumab in Tre...</td>\n",
       "      <td>Stage II Oropharyngeal Squamous Cell Carcinoma...</td>\n",
       "      <td>Change of CD8+ tumor infiltrating lymphocytes,...</td>\n",
       "      <td>NCT03144778</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The incorporation of novel targeted therapies ...</td>\n",
       "      <td>Radiation and Cetuximab Plus Intratumoral EGFR...</td>\n",
       "      <td>Squamous Cell Carcinoma|Head and Neck Cancer</td>\n",
       "      <td>Toxicity Rate, This is a 2-stage clinical tria...</td>\n",
       "      <td>NCT01592721</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The use of nonsurgical periodontal treatment, ...</td>\n",
       "      <td>Laser Biostimulation in Periodontal Treatment</td>\n",
       "      <td>Periodontal Inflammation|Periodontal Diseases</td>\n",
       "      <td>IL-1β level in GCF, IL-1β is a cytokine presen...</td>\n",
       "      <td>NCT04253613</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>OBJECTIVES: The aim of the study is to evaluat...</td>\n",
       "      <td>Radiofrequency Female External Genital Region:...</td>\n",
       "      <td>Dissatisfaction Appearance of the External Gen...</td>\n",
       "      <td>clinical response, To evaluate the clinical re...</td>\n",
       "      <td>NCT02611791</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>The proposed study evaluates the impact and im...</td>\n",
       "      <td>Journey of Life Psychosocial Support Program</td>\n",
       "      <td>Psychological|Child Development|Social Values</td>\n",
       "      <td>Mental health, Changes in mental health sympto...</td>\n",
       "      <td>NCT04817098</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>This study is to find out what role a local in...</td>\n",
       "      <td>Human Intestinal Amino Acid Absorption and the...</td>\n",
       "      <td>Renin-Angiotensin Aldosterone System (RAS)</td>\n",
       "      <td>messenger ribonucleic acid (mRNA) quantificati...</td>\n",
       "      <td>NCT04524494</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>A phase II single arm study of carboplatin and...</td>\n",
       "      <td>A Phase II Study of Docetaxel and Carboplatin ...</td>\n",
       "      <td>Ovarian Epithelial Cancer Recurrent</td>\n",
       "      <td>Safety, Safety will be established by grading ...</td>\n",
       "      <td>NCT02026921</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>This research study is testing the feasibility...</td>\n",
       "      <td>Improving Adherence to ACS Guidelines on Nutri...</td>\n",
       "      <td>Cancer</td>\n",
       "      <td>Determine the feasibility, Feasibility assesse...</td>\n",
       "      <td>NCT04314479</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Brief Summary  \\\n",
       "0    This study is a post-market clinical follow-up...   \n",
       "1    The aim of the study is to evaluate the effica...   \n",
       "2    This phase I trial studies how well durvalumab...   \n",
       "3    The incorporation of novel targeted therapies ...   \n",
       "4    The use of nonsurgical periodontal treatment, ...   \n",
       "..                                                 ...   \n",
       "495  OBJECTIVES: The aim of the study is to evaluat...   \n",
       "496  The proposed study evaluates the impact and im...   \n",
       "497  This study is to find out what role a local in...   \n",
       "498  A phase II single arm study of carboplatin and...   \n",
       "499  This research study is testing the feasibility...   \n",
       "\n",
       "                                           Study Title  \\\n",
       "0          Patient Outcomes Using an Expandable Spacer   \n",
       "1    the Effect of Isosorbide Mononitrate in Reduci...   \n",
       "2    Durvalumab With or Without Tremelimumab in Tre...   \n",
       "3    Radiation and Cetuximab Plus Intratumoral EGFR...   \n",
       "4        Laser Biostimulation in Periodontal Treatment   \n",
       "..                                                 ...   \n",
       "495  Radiofrequency Female External Genital Region:...   \n",
       "496       Journey of Life Psychosocial Support Program   \n",
       "497  Human Intestinal Amino Acid Absorption and the...   \n",
       "498  A Phase II Study of Docetaxel and Carboplatin ...   \n",
       "499  Improving Adherence to ACS Guidelines on Nutri...   \n",
       "\n",
       "                                            Conditions  \\\n",
       "0                            Degenerative Disc Disease   \n",
       "1                                   IUD Insertion Pain   \n",
       "2    Stage II Oropharyngeal Squamous Cell Carcinoma...   \n",
       "3         Squamous Cell Carcinoma|Head and Neck Cancer   \n",
       "4        Periodontal Inflammation|Periodontal Diseases   \n",
       "..                                                 ...   \n",
       "495  Dissatisfaction Appearance of the External Gen...   \n",
       "496      Psychological|Child Development|Social Values   \n",
       "497         Renin-Angiotensin Aldosterone System (RAS)   \n",
       "498                Ovarian Epithelial Cancer Recurrent   \n",
       "499                                             Cancer   \n",
       "\n",
       "                                              Outcomes   NCT Number  \\\n",
       "0    Change in Radiographic Analysis, Global and Se...  NCT03162666   \n",
       "1    pain during IUD insertion, intensity of patien...  NCT04312048   \n",
       "2    Change of CD8+ tumor infiltrating lymphocytes,...  NCT03144778   \n",
       "3    Toxicity Rate, This is a 2-stage clinical tria...  NCT01592721   \n",
       "4    IL-1β level in GCF, IL-1β is a cytokine presen...  NCT04253613   \n",
       "..                                                 ...          ...   \n",
       "495  clinical response, To evaluate the clinical re...  NCT02611791   \n",
       "496  Mental health, Changes in mental health sympto...  NCT04817098   \n",
       "497  messenger ribonucleic acid (mRNA) quantificati...  NCT04524494   \n",
       "498  Safety, Safety will be established by grading ...  NCT02026921   \n",
       "499  Determine the feasibility, Feasibility assesse...  NCT04314479   \n",
       "\n",
       "     Study Status  \n",
       "0               0  \n",
       "1               1  \n",
       "2               1  \n",
       "3               1  \n",
       "4               1  \n",
       "..            ...  \n",
       "495             1  \n",
       "496             1  \n",
       "497             1  \n",
       "498             1  \n",
       "499             1  \n",
       "\n",
       "[500 rows x 6 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Study Status\"] = df[\"Study Status\"].apply(\n",
    "    lambda x: 1 if x == \"COMPLETED\" else 0\n",
    ")\n",
    "\n",
    "test_df[\"Study Status\"] = test_df[\"Study Status\"].apply(\n",
    "    lambda x: 1 if x == \"COMPLETED\" else 0\n",
    ")\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (500, 5) (500,)\n",
      "Testing set shape: (500, 5) (500,)\n"
     ]
    }
   ],
   "source": [
    "# Separate features (X) and target variable (y) for the training set\n",
    "X_train = df.drop('Study Status', axis=1)\n",
    "y_train = df['Study Status']\n",
    "\n",
    "# Separate features (X) and target variable (y) for the testing set\n",
    "X_test = test_df.drop('Study Status', axis=1)\n",
    "y_test = test_df['Study Status']\n",
    "\n",
    "# Print the shapes of the resulting sets\n",
    "print(\"Training set shape:\", X_train.shape, y_train.shape)\n",
    "print(\"Testing set shape:\", X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Brief Summary</th>\n",
       "      <th>Study Title</th>\n",
       "      <th>Conditions</th>\n",
       "      <th>Outcomes</th>\n",
       "      <th>NCT Number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The hypothesis of this study is use of CytoSor...</td>\n",
       "      <td>Efficacy Study of CytoSorb Hemoperfusion Devic...</td>\n",
       "      <td>Acute Respiratory Distress Syndrome|Acute Lung...</td>\n",
       "      <td>Relative IL-6 levels as a percent (%) of basel...</td>\n",
       "      <td>NCT00559130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The primary purpose of this study is to find o...</td>\n",
       "      <td>Safety and Tolerability Study of AZD7762 in Co...</td>\n",
       "      <td>Cancer|Solid Tumors|Advanced Solid Malignancies</td>\n",
       "      <td>Assessment of adverse events (based on CTCAE v...</td>\n",
       "      <td>NCT00937664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>To study the impact of 3 day exposure to atorv...</td>\n",
       "      <td>Does Atorvastatin Reduce Ischemia-Reperfusion ...</td>\n",
       "      <td>Ischemia Reperfusion Injury|Cardiovascular Dis...</td>\n",
       "      <td>Annexin A 5 targeting in the non dominant then...</td>\n",
       "      <td>NCT00441597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The purpose of this study is to identify the f...</td>\n",
       "      <td>Comparison of Dynamic Radiographs in Determini...</td>\n",
       "      <td>Adolescent Idiopathic Scoliosis</td>\n",
       "      <td>Investigate the flexibility equivalence of dif...</td>\n",
       "      <td>NCT03296228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The proposed protocol is a double-blind, place...</td>\n",
       "      <td>A Placebo-Controlled Study of Mixed Amphetamin...</td>\n",
       "      <td>Cocaine Dependence</td>\n",
       "      <td>Three Weeks of Continuous Cocaine Abstinence a...</td>\n",
       "      <td>NCT00421603</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       Brief Summary  \\\n",
       "0  The hypothesis of this study is use of CytoSor...   \n",
       "1  The primary purpose of this study is to find o...   \n",
       "2  To study the impact of 3 day exposure to atorv...   \n",
       "3  The purpose of this study is to identify the f...   \n",
       "4  The proposed protocol is a double-blind, place...   \n",
       "\n",
       "                                         Study Title  \\\n",
       "0  Efficacy Study of CytoSorb Hemoperfusion Devic...   \n",
       "1  Safety and Tolerability Study of AZD7762 in Co...   \n",
       "2  Does Atorvastatin Reduce Ischemia-Reperfusion ...   \n",
       "3  Comparison of Dynamic Radiographs in Determini...   \n",
       "4  A Placebo-Controlled Study of Mixed Amphetamin...   \n",
       "\n",
       "                                          Conditions  \\\n",
       "0  Acute Respiratory Distress Syndrome|Acute Lung...   \n",
       "1    Cancer|Solid Tumors|Advanced Solid Malignancies   \n",
       "2  Ischemia Reperfusion Injury|Cardiovascular Dis...   \n",
       "3                    Adolescent Idiopathic Scoliosis   \n",
       "4                                 Cocaine Dependence   \n",
       "\n",
       "                                            Outcomes   NCT Number  \n",
       "0  Relative IL-6 levels as a percent (%) of basel...  NCT00559130  \n",
       "1  Assessment of adverse events (based on CTCAE v...  NCT00937664  \n",
       "2  Annexin A 5 targeting in the non dominant then...  NCT00441597  \n",
       "3  Investigate the flexibility equivalence of dif...  NCT03296228  \n",
       "4  Three Weeks of Continuous Cocaine Abstinence a...  NCT00421603  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 5)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "1    0\n",
       "2    1\n",
       "3    1\n",
       "4    1\n",
       "Name: Study Status, dtype: int64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500,)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train Shape: (500, 5)\n",
      "y_train Shape: (500,)\n",
      "\n",
      "X_train Index Sample: [0, 1, 2, 3, 4]\n",
      "\n",
      "y_train Index Sample: [0, 1, 2, 3, 4]\n",
      "\n",
      "X_train Type: <class 'pandas.core.frame.DataFrame'>\n",
      "y_train Type: <class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train Shape:\", X_train.shape)  # Expected: (206061, 2)\n",
    "print(\"y_train Shape:\", y_train.shape)  # Expected: (206061, 1) or (206061,)\n",
    "\n",
    "print(\"\\nX_train Index Sample:\", X_train.index[:5].tolist())\n",
    "print(\"\\ny_train Index Sample:\", y_train.index[:5].tolist())\n",
    "\n",
    "print(\"\\nX_train Type:\", type(X_train))\n",
    "print(\"y_train Type:\", type(y_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                       Brief Summary  \\\n",
      "0  The hypothesis of this study is use of CytoSor...   \n",
      "1  The primary purpose of this study is to find o...   \n",
      "2  To study the impact of 3 day exposure to atorv...   \n",
      "3  The purpose of this study is to identify the f...   \n",
      "4  The proposed protocol is a double-blind, place...   \n",
      "\n",
      "                                         Study Title  \\\n",
      "0  Efficacy Study of CytoSorb Hemoperfusion Devic...   \n",
      "1  Safety and Tolerability Study of AZD7762 in Co...   \n",
      "2  Does Atorvastatin Reduce Ischemia-Reperfusion ...   \n",
      "3  Comparison of Dynamic Radiographs in Determini...   \n",
      "4  A Placebo-Controlled Study of Mixed Amphetamin...   \n",
      "\n",
      "                                          Conditions  \\\n",
      "0  Acute Respiratory Distress Syndrome|Acute Lung...   \n",
      "1    Cancer|Solid Tumors|Advanced Solid Malignancies   \n",
      "2  Ischemia Reperfusion Injury|Cardiovascular Dis...   \n",
      "3                    Adolescent Idiopathic Scoliosis   \n",
      "4                                 Cocaine Dependence   \n",
      "\n",
      "                                            Outcomes   NCT Number  Label  \n",
      "0  Relative IL-6 levels as a percent (%) of basel...  NCT00559130      1  \n",
      "1  Assessment of adverse events (based on CTCAE v...  NCT00937664      0  \n",
      "2  Annexin A 5 targeting in the non dominant then...  NCT00441597      1  \n",
      "3  Investigate the flexibility equivalence of dif...  NCT03296228      1  \n",
      "4  Three Weeks of Continuous Cocaine Abstinence a...  NCT00421603      1  \n"
     ]
    }
   ],
   "source": [
    "X_train = X_train.reset_index(drop=True)\n",
    "y_train = y_train.reset_index(drop=True)\n",
    "\n",
    "# Convert y_train to DataFrame if it's a Series or NumPy array\n",
    "if isinstance(y_train, pd.Series):\n",
    "    y_train = y_train.to_frame()\n",
    "elif isinstance(y_train, np.ndarray):\n",
    "    y_train = pd.DataFrame(y_train, columns=[\"Label\"])\n",
    "\n",
    "# Explicitly rename column\n",
    "y_train.columns = [\"Label\"]\n",
    "\n",
    "# Concatenate X_train and y_train along columns\n",
    "train_data = pd.concat([X_train, y_train], axis=1)\n",
    "\n",
    "# Print first rows to confirm merge\n",
    "print(train_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label\n",
      "1    431\n",
      "0     69\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Count occurrences of each label\n",
    "label_counts = train_data[\"Label\"].value_counts()\n",
    "\n",
    "# Print results\n",
    "print(label_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Class Distribution:\n",
      " Label\n",
      "0    517\n",
      "1    431\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "majority_class = train_data[train_data[\"Label\"] == 1]  # 176,886 rows\n",
    "minority_class = train_data[train_data[\"Label\"] == 0]  # 29,175 rows\n",
    "\n",
    "# Calculate the desired oversampling ratio\n",
    "oversampling_ratio = 1.2\n",
    "\n",
    "# Calculate the target size for the minority class\n",
    "target_minority_size = int(len(majority_class) * oversampling_ratio)\n",
    "\n",
    "# Calculate how many times to duplicate the minority class\n",
    "duplication_factor = target_minority_size // len(minority_class)  # Integer factor\n",
    "remainder = target_minority_size % len(minority_class)  # Extra rows needed\n",
    "\n",
    "# Duplicate the minority class\n",
    "balanced_minority_class = pd.concat([minority_class] * duplication_factor, ignore_index=True)\n",
    "\n",
    "# Add extra samples to match the target size\n",
    "extra_samples = minority_class.sample(n=remainder, replace=True, random_state=42)  # Resample to match exact count\n",
    "\n",
    "# Merge the newly balanced data\n",
    "balanced_train_data = pd.concat([majority_class, balanced_minority_class, extra_samples], ignore_index=True)\n",
    "\n",
    "# Shuffle the dataset\n",
    "balanced_train_data = balanced_train_data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Print new class counts\n",
    "print(\"New Class Distribution:\\n\", balanced_train_data[\"Label\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Brief Summary</th>\n",
       "      <th>Study Title</th>\n",
       "      <th>Conditions</th>\n",
       "      <th>Outcomes</th>\n",
       "      <th>NCT Number</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This study is a Post Market Clinical Follow up...</td>\n",
       "      <td>Post Market Clinical Follow-Up of the Zimmer S...</td>\n",
       "      <td>Osteoarthritis, Hip|Fracture of Hip|Avascular ...</td>\n",
       "      <td>Implant Survival, Represents the implants that...</td>\n",
       "      <td>NCT04079114</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Verruca vulgaris, otherwise known as the commo...</td>\n",
       "      <td>The Purpose of This Study is to Determine Whet...</td>\n",
       "      <td>Warts</td>\n",
       "      <td>Safety [SEP] Warts [SEP] Resolution of Common ...</td>\n",
       "      <td>NCT00546611</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This study will be conducted as a randomized, ...</td>\n",
       "      <td>Trial Comparing Effects of Xyrem Taken Orally ...</td>\n",
       "      <td>Narcolepsy</td>\n",
       "      <td>Daytime Sleep Latency as Measured by the Maint...</td>\n",
       "      <td>NCT00066170</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This is a research study on Altitude Illness. ...</td>\n",
       "      <td>Prevention of Altitude Illness With Non-steroi...</td>\n",
       "      <td>Altitude Sickness</td>\n",
       "      <td>Acute Mountain Sickness, Lake Louise Criteria ...</td>\n",
       "      <td>NCT01171794</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This is a study in people with an eye disease ...</td>\n",
       "      <td>A Study to Test Different Doses of BI 836880 i...</td>\n",
       "      <td>Wet Macular Degeneration</td>\n",
       "      <td>Single Rising Dose (SRD) part: Number of patie...</td>\n",
       "      <td>NCT03861234</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       Brief Summary  \\\n",
       "0  This study is a Post Market Clinical Follow up...   \n",
       "1  Verruca vulgaris, otherwise known as the commo...   \n",
       "2  This study will be conducted as a randomized, ...   \n",
       "3  This is a research study on Altitude Illness. ...   \n",
       "4  This is a study in people with an eye disease ...   \n",
       "\n",
       "                                         Study Title  \\\n",
       "0  Post Market Clinical Follow-Up of the Zimmer S...   \n",
       "1  The Purpose of This Study is to Determine Whet...   \n",
       "2  Trial Comparing Effects of Xyrem Taken Orally ...   \n",
       "3  Prevention of Altitude Illness With Non-steroi...   \n",
       "4  A Study to Test Different Doses of BI 836880 i...   \n",
       "\n",
       "                                          Conditions  \\\n",
       "0  Osteoarthritis, Hip|Fracture of Hip|Avascular ...   \n",
       "1                                              Warts   \n",
       "2                                         Narcolepsy   \n",
       "3                                  Altitude Sickness   \n",
       "4                           Wet Macular Degeneration   \n",
       "\n",
       "                                            Outcomes   NCT Number  Label  \n",
       "0  Implant Survival, Represents the implants that...  NCT04079114      0  \n",
       "1  Safety [SEP] Warts [SEP] Resolution of Common ...  NCT00546611      0  \n",
       "2  Daytime Sleep Latency as Measured by the Maint...  NCT00066170      1  \n",
       "3  Acute Mountain Sickness, Lake Louise Criteria ...  NCT01171794      1  \n",
       "4  Single Rising Dose (SRD) part: Number of patie...  NCT03861234      1  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "balanced_train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train Shape: (948, 5)\n",
      "y_train Shape: (948,)\n"
     ]
    }
   ],
   "source": [
    "# Separate features (X_train) and target labels (y_train)\n",
    "X_train = balanced_train_data.drop(columns=[\"Label\"])  # Drop the label column\n",
    "y_train = balanced_train_data[\"Label\"]  # Keep only the label column\n",
    "\n",
    "# Display shapes to confirm correctness\n",
    "print(\"X_train Shape:\", X_train.shape)\n",
    "print(\"y_train Shape:\", y_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Brief Summary</th>\n",
       "      <th>Study Title</th>\n",
       "      <th>Conditions</th>\n",
       "      <th>Outcomes</th>\n",
       "      <th>NCT Number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This study is a Post Market Clinical Follow up...</td>\n",
       "      <td>Post Market Clinical Follow-Up of the Zimmer S...</td>\n",
       "      <td>Osteoarthritis, Hip|Fracture of Hip|Avascular ...</td>\n",
       "      <td>Implant Survival, Represents the implants that...</td>\n",
       "      <td>NCT04079114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Verruca vulgaris, otherwise known as the commo...</td>\n",
       "      <td>The Purpose of This Study is to Determine Whet...</td>\n",
       "      <td>Warts</td>\n",
       "      <td>Safety [SEP] Warts [SEP] Resolution of Common ...</td>\n",
       "      <td>NCT00546611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This study will be conducted as a randomized, ...</td>\n",
       "      <td>Trial Comparing Effects of Xyrem Taken Orally ...</td>\n",
       "      <td>Narcolepsy</td>\n",
       "      <td>Daytime Sleep Latency as Measured by the Maint...</td>\n",
       "      <td>NCT00066170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This is a research study on Altitude Illness. ...</td>\n",
       "      <td>Prevention of Altitude Illness With Non-steroi...</td>\n",
       "      <td>Altitude Sickness</td>\n",
       "      <td>Acute Mountain Sickness, Lake Louise Criteria ...</td>\n",
       "      <td>NCT01171794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This is a study in people with an eye disease ...</td>\n",
       "      <td>A Study to Test Different Doses of BI 836880 i...</td>\n",
       "      <td>Wet Macular Degeneration</td>\n",
       "      <td>Single Rising Dose (SRD) part: Number of patie...</td>\n",
       "      <td>NCT03861234</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       Brief Summary  \\\n",
       "0  This study is a Post Market Clinical Follow up...   \n",
       "1  Verruca vulgaris, otherwise known as the commo...   \n",
       "2  This study will be conducted as a randomized, ...   \n",
       "3  This is a research study on Altitude Illness. ...   \n",
       "4  This is a study in people with an eye disease ...   \n",
       "\n",
       "                                         Study Title  \\\n",
       "0  Post Market Clinical Follow-Up of the Zimmer S...   \n",
       "1  The Purpose of This Study is to Determine Whet...   \n",
       "2  Trial Comparing Effects of Xyrem Taken Orally ...   \n",
       "3  Prevention of Altitude Illness With Non-steroi...   \n",
       "4  A Study to Test Different Doses of BI 836880 i...   \n",
       "\n",
       "                                          Conditions  \\\n",
       "0  Osteoarthritis, Hip|Fracture of Hip|Avascular ...   \n",
       "1                                              Warts   \n",
       "2                                         Narcolepsy   \n",
       "3                                  Altitude Sickness   \n",
       "4                           Wet Macular Degeneration   \n",
       "\n",
       "                                            Outcomes   NCT Number  \n",
       "0  Implant Survival, Represents the implants that...  NCT04079114  \n",
       "1  Safety [SEP] Warts [SEP] Resolution of Common ...  NCT00546611  \n",
       "2  Daytime Sleep Latency as Measured by the Maint...  NCT00066170  \n",
       "3  Acute Mountain Sickness, Lake Louise Criteria ...  NCT01171794  \n",
       "4  Single Rising Dose (SRD) part: Number of patie...  NCT03861234  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0\n",
       "1    0\n",
       "2    1\n",
       "3    1\n",
       "4    1\n",
       "Name: Label, dtype: int64"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train.to_csv(folder_path + \"X_train.csv\", index=False)\n",
    "# X_test.to_csv(folder_path + \"X_test.csv\", index=False)\n",
    "# y_train.to_csv(folder_path + \"y_train.csv\", index=False)\n",
    "# y_test.to_csv(folder_path + \"y_test.csv\", index=False)\n",
    "# X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: False\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[54], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA Available:\u001b[39m\u001b[38;5;124m\"\u001b[39m, torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available())\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCurrent Device:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDevice Name:\u001b[39m\u001b[38;5;124m\"\u001b[39m, torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mget_device_name(\u001b[38;5;241m0\u001b[39m))\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPyTorch using CUDA:\u001b[39m\u001b[38;5;124m\"\u001b[39m, torch\u001b[38;5;241m.\u001b[39mbackends\u001b[38;5;241m.\u001b[39mcudnn\u001b[38;5;241m.\u001b[39menabled)\n",
      "File \u001b[1;32mc:\\Users\\mogal\\anaconda3\\envs\\NEST\\lib\\site-packages\\torch\\cuda\\__init__.py:878\u001b[0m, in \u001b[0;36mcurrent_device\u001b[1;34m()\u001b[0m\n\u001b[0;32m    876\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcurrent_device\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[0;32m    877\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Return the index of a currently selected device.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 878\u001b[0m     \u001b[43m_lazy_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    879\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_cuda_getDevice()\n",
      "File \u001b[1;32mc:\\Users\\mogal\\anaconda3\\envs\\NEST\\lib\\site-packages\\torch\\cuda\\__init__.py:305\u001b[0m, in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    300\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    301\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    302\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiprocessing, you must use the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m start method\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    303\u001b[0m     )\n\u001b[0;32m    304\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_cuda_getDeviceCount\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 305\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch not compiled with CUDA enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    306\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    307\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[0;32m    308\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    309\u001b[0m     )\n",
      "\u001b[1;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "print(\"Current Device:\", torch.cuda.current_device())\n",
    "print(\"Device Name:\", torch.cuda.get_device_name(0))\n",
    "print(\"PyTorch using CUDA:\", torch.backends.cudnn.enabled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at emilyalsentzer/Bio_ClinicalBERT and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\mogal\\anaconda3\\envs\\NEST\\lib\\site-packages\\transformers\\training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is on: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▊     | 70/144 [03:35<03:47,  3.07s/it]\n",
      " 21%|██        | 10/48 [00:16<00:59,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7001, 'grad_norm': 2.827866792678833, 'learning_rate': 1.5833333333333333e-05, 'epoch': 0.21}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 20/48 [00:31<00:42,  1.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6763, 'grad_norm': 3.411410093307495, 'learning_rate': 1.1666666666666668e-05, 'epoch': 0.42}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▎   | 30/48 [00:46<00:27,  1.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6697, 'grad_norm': 2.7578377723693848, 'learning_rate': 7.500000000000001e-06, 'epoch': 0.62}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 40/48 [01:01<00:12,  1.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6281, 'grad_norm': 3.3203423023223877, 'learning_rate': 3.3333333333333333e-06, 'epoch': 0.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \n",
      "100%|██████████| 48/48 [01:19<00:00,  1.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6134178638458252, 'eval_runtime': 5.1077, 'eval_samples_per_second': 37.199, 'eval_steps_per_second': 2.349, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [01:21<00:00,  1.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 81.2985, 'train_samples_per_second': 9.324, 'train_steps_per_second': 0.59, 'train_loss': 0.6605633397897085, 'epoch': 1.0}\n",
      "Fine-tuned BioBERT model saved at './fine_tuned_biobert_v3'\n",
      "Model is on: cpu\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # Ensure only GPU 0 is used\n",
    "\n",
    "\n",
    "# Check if GPU is available and set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Custom Dataset Class\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Load Data\n",
    "texts = X_train['Study Title'].tolist()\n",
    "labels = y_train.tolist()\n",
    "\n",
    "# Split Data into Training and Validation Sets\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    texts, labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Load BioBERT Tokenizer and Model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\", num_labels=2)\n",
    "\n",
    "# Move model to GPU\n",
    "model.to(device)\n",
    "\n",
    "# Check model device\n",
    "print(\"Model is on:\", next(model.parameters()).device)\n",
    "\n",
    "# Create Datasets\n",
    "train_dataset = TextDataset(train_texts, train_labels, tokenizer, max_length=56)\n",
    "val_dataset = TextDataset(val_texts, val_labels, tokenizer, max_length=56)\n",
    "\n",
    "# Training Arguments (Ensure GPU usage)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,  # Reduce if OOM occurs\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=epochs,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    fp16=True,  # Enable mixed precision training\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset\n",
    ")\n",
    "\n",
    "# Fine-Tune Model\n",
    "trainer.train()\n",
    "\n",
    "# Save Fine-Tuned Model\n",
    "model.save_pretrained(folder_path +\"study_title_fine_tuned_clinicalbiobert_v3\")\n",
    "tokenizer.save_pretrained(folder_path + \"study_title_fine_tuned_clinicalbiobert_v3\")\n",
    "\n",
    "print(f\"Fine-tuned BioBERT model saved at {folder_path +\"study_title_fine_tuned_clinicalbiobert_v3\"}\")\n",
    "print(\"Model is on:\", next(model.parameters()).device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix for E:\\Case Comp\\NEST\\Training\\study_title_clinicalbiobert_predictions_train.csv:\n",
      "[[ 67   2]\n",
      " [244 187]]\n",
      "\n",
      "Classification Report for E:\\Case Comp\\NEST\\Training\\study_title_clinicalbiobert_predictions_train.csv:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.22      0.97      0.35        69\n",
      "     Class 1       0.99      0.43      0.60       431\n",
      "\n",
      "    accuracy                           0.51       500\n",
      "   macro avg       0.60      0.70      0.48       500\n",
      "weighted avg       0.88      0.51      0.57       500\n",
      "\n",
      "Predictions saved to E:\\Case Comp\\NEST\\Training\\study_title_clinicalbiobert_predictions_train.csv\n",
      "\n",
      "Confusion Matrix for E:\\Case Comp\\NEST\\Training\\study_title_clinicalbiobert_predictions_test.csv:\n",
      "[[ 39  16]\n",
      " [307 138]]\n",
      "\n",
      "Classification Report for E:\\Case Comp\\NEST\\Training\\study_title_clinicalbiobert_predictions_test.csv:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.11      0.71      0.19        55\n",
      "     Class 1       0.90      0.31      0.46       445\n",
      "\n",
      "    accuracy                           0.35       500\n",
      "   macro avg       0.50      0.51      0.33       500\n",
      "weighted avg       0.81      0.35      0.43       500\n",
      "\n",
      "Predictions saved to E:\\Case Comp\\NEST\\Training\\study_title_clinicalbiobert_predictions_test.csv\n"
     ]
    }
   ],
   "source": [
    "# Load the fine-tuned model and tokenizer\n",
    "model_path = folder_path + \"study_title_fine_tuned_clinicalbiobert_v3\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()  # Set model to evaluation mode\n",
    "\n",
    "# Tokenize texts\n",
    "def encode_texts(texts, tokenizer, max_length=56):\n",
    "    encodings = tokenizer(\n",
    "        texts,\n",
    "        max_length=max_length,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    return encodings\n",
    "\n",
    "# Inference and evaluation function\n",
    "def evaluate_model(dataframe, tokenizer, model, device, output_csv_name, text_column, max_length=56, batch_size=8):\n",
    "    texts = dataframe[text_column].tolist()\n",
    "    nct_numbers = dataframe[\"NCT Number\"].tolist()\n",
    "    labels = dataframe[\"Study Status\"].tolist()\n",
    "\n",
    "    # Tokenize data\n",
    "    encodings = encode_texts(texts, tokenizer, max_length)\n",
    "    input_ids = encodings[\"input_ids\"].to(device)\n",
    "    attention_mask = encodings[\"attention_mask\"].to(device)\n",
    "\n",
    "    # # Free unused memory\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    torch.cuda.reset_accumulated_memory_stats()\n",
    "\n",
    "    # Create DataLoader\n",
    "    dataset = TensorDataset(input_ids, attention_mask)\n",
    "    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    predictions = []\n",
    "    prediction_probs = []\n",
    "\n",
    "    # Run inference\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            batch_input_ids, batch_attention_mask = batch\n",
    "            batch_input_ids = batch_input_ids.to(device)\n",
    "            batch_attention_mask = batch_attention_mask.to(device)\n",
    "\n",
    "            outputs = model(input_ids=batch_input_ids, attention_mask=batch_attention_mask)\n",
    "            logits = outputs.logits\n",
    "            probs = torch.nn.functional.softmax(logits, dim=1)  # Get prediction probabilities\n",
    "\n",
    "            batch_predictions = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "            batch_probs = probs.cpu().numpy()\n",
    "\n",
    "            predictions.extend(batch_predictions)\n",
    "            prediction_probs.extend(batch_probs)\n",
    "\n",
    "    # Convert true labels to NumPy\n",
    "    true_labels = torch.tensor(labels).numpy()\n",
    "\n",
    "    # Compute Confusion Matrix\n",
    "    conf_matrix = confusion_matrix(true_labels, predictions)\n",
    "    print(f\"\\nConfusion Matrix for {output_csv_name}:\")\n",
    "    print(conf_matrix)\n",
    "\n",
    "    # Compute Classification Report\n",
    "    class_report = classification_report(true_labels, predictions, target_names=[\"Class 0\", \"Class 1\"])\n",
    "    print(f\"\\nClassification Report for {output_csv_name}:\")\n",
    "    print(class_report)\n",
    "\n",
    "    # Save predictions and probabilities to a DataFrame\n",
    "    pred_df = pd.DataFrame({\n",
    "        \"NCT Number\": nct_numbers,\n",
    "        \"True Label\": true_labels,\n",
    "        \"Predicted Label\": predictions,\n",
    "        \"Probability Class 0\": [prob[0] for prob in prediction_probs],\n",
    "        \"Probability Class 1\": [prob[1] for prob in prediction_probs]\n",
    "    })\n",
    "\n",
    "    # Save to CSV\n",
    "    pred_df.to_csv(output_csv_name, index=False)\n",
    "    print(f\"Predictions saved to {output_csv_name}\")\n",
    "\n",
    "# Evaluate on training data (df)\n",
    "evaluate_model(\n",
    "    dataframe=df,\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    device=device,\n",
    "    output_csv_name= folder_path + \"study_title_clinicalbiobert_predictions_train.csv\",\n",
    "    text_column=\"Study Title\",\n",
    "    max_length=128  # Set max_length to 128 for training data\n",
    ")\n",
    "\n",
    "# Evaluate on testing data (test_df)\n",
    "evaluate_model(\n",
    "    dataframe=test_df,\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    device=device,\n",
    "    output_csv_name= folder_path + \"study_title_clinicalbiobert_predictions_test.csv\",\n",
    "    text_column=\"Study Title\",\n",
    "    max_length=128  # Set max_length to 128 for testing data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at emilyalsentzer/Bio_ClinicalBERT and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\mogal\\anaconda3\\envs\\NEST\\lib\\site-packages\\transformers\\training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is on: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 10/48 [00:34<02:08,  3.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6914, 'grad_norm': 2.0106403827667236, 'learning_rate': 1.5833333333333333e-05, 'epoch': 0.21}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 20/48 [01:06<01:31,  3.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7029, 'grad_norm': 2.805807113647461, 'learning_rate': 1.1666666666666668e-05, 'epoch': 0.42}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▎   | 30/48 [01:41<01:00,  3.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6872, 'grad_norm': 2.1524529457092285, 'learning_rate': 7.500000000000001e-06, 'epoch': 0.62}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 40/48 [02:13<00:26,  3.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6619, 'grad_norm': 2.487837553024292, 'learning_rate': 3.3333333333333333e-06, 'epoch': 0.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [02:38<00:00,  2.70s/it]\n",
      "100%|██████████| 48/48 [02:51<00:00,  2.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6464076042175293, 'eval_runtime': 11.8255, 'eval_samples_per_second': 16.067, 'eval_steps_per_second': 1.015, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [02:52<00:00,  3.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 172.7629, 'train_samples_per_second': 4.388, 'train_steps_per_second': 0.278, 'train_loss': 0.6787485182285309, 'epoch': 1.0}\n",
      "Fine-tuned BioBERT model saved at './fine_tuned_biobert_v3'\n",
      "Model is on: cpu\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # Ensure only GPU 0 is used\n",
    "\n",
    "\n",
    "# Check if GPU is available and set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Custom Dataset Class\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Load Data\n",
    "texts = X_train['Brief Summary'].tolist()\n",
    "labels = y_train.tolist()\n",
    "\n",
    "# Split Data into Training and Validation Sets\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    texts, labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Load BioBERT Tokenizer and Model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\", num_labels=2)\n",
    "\n",
    "# Move model to GPU\n",
    "model.to(device)\n",
    "\n",
    "# Check model device\n",
    "print(\"Model is on:\", next(model.parameters()).device)\n",
    "\n",
    "# Create Datasets\n",
    "train_dataset = TextDataset(train_texts, train_labels, tokenizer, max_length=128)\n",
    "val_dataset = TextDataset(val_texts, val_labels, tokenizer, max_length=128)\n",
    "\n",
    "# Training Arguments (Ensure GPU usage)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,  # Reduce if OOM occurs\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=epochs,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    fp16=True,  # Enable mixed precision training\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset\n",
    ")\n",
    "\n",
    "# Fine-Tune Model\n",
    "trainer.train()\n",
    "\n",
    "# Save Fine-Tuned Model\n",
    "model.save_pretrained(folder_path + \"brief_summary_fine_tuned_clinicalbiobert_v3\")\n",
    "tokenizer.save_pretrained(folder_path + \"brief_summary_fine_tuned_clinicalbiobert_v3\")\n",
    "\n",
    "print(\"Fine-tuned BioBERT model saved at './fine_tuned_biobert_v3'\")\n",
    "print(\"Model is on:\", next(model.parameters()).device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix for brief_summary_clinicalbiobert_predictions_test.csv:\n",
      "[[ 4047  5161]\n",
      " [10419 44768]]\n",
      "\n",
      "Classification Report for brief_summary_clinicalbiobert_predictions_test.csv:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.28      0.44      0.34      9208\n",
      "     Class 1       0.90      0.81      0.85     55187\n",
      "\n",
      "    accuracy                           0.76     64395\n",
      "   macro avg       0.59      0.63      0.60     64395\n",
      "weighted avg       0.81      0.76      0.78     64395\n",
      "\n",
      "Predictions saved to brief_summary_clinicalbiobert_predictions_test.csv\n",
      "\n",
      "Confusion Matrix for brief_summary_clinicalbiobert_predictions_train.csv:\n",
      "[[ 36142    192]\n",
      " [ 21837 199406]]\n",
      "\n",
      "Classification Report for brief_summary_clinicalbiobert_predictions_train.csv:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.62      0.99      0.77     36334\n",
      "     Class 1       1.00      0.90      0.95    221243\n",
      "\n",
      "    accuracy                           0.91    257577\n",
      "   macro avg       0.81      0.95      0.86    257577\n",
      "weighted avg       0.95      0.91      0.92    257577\n",
      "\n",
      "Predictions saved to brief_summary_clinicalbiobert_predictions_train.csv\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on testing data (test_df) using \"Brief Summary\" column\n",
    "evaluate_model(\n",
    "    dataframe=test_df,\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    device=device,\n",
    "    output_csv_name=folder_path + \"brief_summary_clinicalbiobert_predictions_test.csv\",\n",
    "    text_column=\"Brief Summary\",\n",
    "    max_length=128  # Set max_length to 128\n",
    ")\n",
    "\n",
    "# Evaluate on training data (df) using \"Brief Summary\" column\n",
    "evaluate_model(\n",
    "    dataframe=df,\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    device=device,\n",
    "    output_csv_name=folder_path + \"brief_summary_clinicalbiobert_predictions_train.csv\",\n",
    "    text_column=\"Brief Summary\",\n",
    "    max_length=128  # Set max_length to 128\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at emilyalsentzer/Bio_ClinicalBERT and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is on: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sriha\\anaconda3\\envs\\nest\\lib\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='73011' max='73011' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [73011/73011 1:45:42, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.685700</td>\n",
       "      <td>0.648017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.718600</td>\n",
       "      <td>0.643998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.652500</td>\n",
       "      <td>0.636457</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuned BioBERT model saved at './fine_tuned_biobert_v3'\n",
      "Model is on: cuda:0\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # Ensure only GPU 0 is used\n",
    "\n",
    "\n",
    "# Check if GPU is available and set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Custom Dataset Class\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Load Data\n",
    "texts = X_train['Conditions'].tolist()\n",
    "labels = y_train.tolist()\n",
    "\n",
    "# Split Data into Training and Validation Sets\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    texts, labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Load BioBERT Tokenizer and Model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\", num_labels=2)\n",
    "\n",
    "# Move model to GPU\n",
    "model.to(device)\n",
    "\n",
    "# Check model device\n",
    "print(\"Model is on:\", next(model.parameters()).device)\n",
    "\n",
    "# Create Datasets\n",
    "train_dataset = TextDataset(train_texts, train_labels, tokenizer, max_length=4)\n",
    "val_dataset = TextDataset(val_texts, val_labels, tokenizer, max_length=4)\n",
    "\n",
    "# Training Arguments (Ensure GPU usage)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,  # Reduce if OOM occurs\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=epochs,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    fp16=True,  # Enable mixed precision training\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset\n",
    ")\n",
    "\n",
    "# Fine-Tune Model\n",
    "trainer.train()\n",
    "\n",
    "# Save Fine-Tuned Model\n",
    "model.save_pretrained(folder_path + \"Conditions_fine_tuned_clinicalbiobert_v3\")\n",
    "tokenizer.save_pretrained(folder_path + \"Conditions_fine_tuned_clinicalbiobert_v3\")\n",
    "\n",
    "print(\"Fine-tuned BioBERT model saved at './fine_tuned_biobert_v3'\")\n",
    "print(\"Model is on:\", next(model.parameters()).device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix for conditions_clinicalbiobert_predictions_test.csv:\n",
      "[[ 6627  2581]\n",
      " [29984 25203]]\n",
      "\n",
      "Classification Report for conditions_clinicalbiobert_predictions_test.csv:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.18      0.72      0.29      9208\n",
      "     Class 1       0.91      0.46      0.61     55187\n",
      "\n",
      "    accuracy                           0.49     64395\n",
      "   macro avg       0.54      0.59      0.45     64395\n",
      "weighted avg       0.80      0.49      0.56     64395\n",
      "\n",
      "Predictions saved to conditions_clinicalbiobert_predictions_test.csv\n",
      "\n",
      "Confusion Matrix for conditions_clinicalbiobert_predictions_train.csv:\n",
      "[[ 28641   7693]\n",
      " [118027 103216]]\n",
      "\n",
      "Classification Report for conditions_clinicalbiobert_predictions_train.csv:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.20      0.79      0.31     36334\n",
      "     Class 1       0.93      0.47      0.62    221243\n",
      "\n",
      "    accuracy                           0.51    257577\n",
      "   macro avg       0.56      0.63      0.47    257577\n",
      "weighted avg       0.83      0.51      0.58    257577\n",
      "\n",
      "Predictions saved to conditions_clinicalbiobert_predictions_train.csv\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on testing data (test_df) using \"Conditions\" column\n",
    "evaluate_model(\n",
    "    dataframe=test_df,\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    device=device,\n",
    "    output_csv_name=folder_path + \"conditions_clinicalbiobert_predictions_test.csv\",\n",
    "    text_column=\"Conditions\",\n",
    "    max_length=4  # Specific max_length for \"Conditions\"\n",
    ")\n",
    "\n",
    "# Evaluate on training data (df) using \"Conditions\" column\n",
    "evaluate_model(\n",
    "    dataframe=df,\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    device=device,\n",
    "    output_csv_name=folder_path + \"conditions_clinicalbiobert_predictions_train.csv\",\n",
    "    text_column=\"Conditions\",\n",
    "    max_length=4  # Specific max_length for \"Conditions\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at emilyalsentzer/Bio_ClinicalBERT and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is on: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sriha\\anaconda3\\envs\\nest\\lib\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='73011' max='73011' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [73011/73011 3:07:39, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.495000</td>\n",
       "      <td>0.399919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.276500</td>\n",
       "      <td>0.273772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.227100</td>\n",
       "      <td>0.352270</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuned BioBERT model saved at './fine_tuned_biobert_v3'\n",
      "Model is on: cuda:0\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # Ensure only GPU 0 is used\n",
    "\n",
    "\n",
    "# Check if GPU is available and set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Custom Dataset Class\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Load Data\n",
    "texts = X_train['Outcomes'].tolist()\n",
    "labels = y_train.tolist()\n",
    "\n",
    "# Split Data into Training and Validation Sets\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    texts, labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Load BioBERT Tokenizer and Model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\", num_labels=2)\n",
    "\n",
    "# Move model to GPU\n",
    "model.to(device)\n",
    "\n",
    "# Check model device\n",
    "print(\"Model is on:\", next(model.parameters()).device)\n",
    "\n",
    "# Create Datasets\n",
    "train_dataset = TextDataset(train_texts, train_labels, tokenizer, max_length=128)\n",
    "val_dataset = TextDataset(val_texts, val_labels, tokenizer, max_length=128)\n",
    "\n",
    "# Training Arguments (Ensure GPU usage)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,  # Reduce if OOM occurs\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=epochs,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    fp16=True,  # Enable mixed precision training\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset\n",
    ")\n",
    "\n",
    "# Fine-Tune Model\n",
    "trainer.train()\n",
    "\n",
    "# Save Fine-Tuned Model\n",
    "model.save_pretrained(folder_path + \"Outcomes_fine_tuned_clinicalbiobert_v3\")\n",
    "tokenizer.save_pretrained(folder_path + \"Outcomes_fine_tuned_clinicalbiobert_v3\")\n",
    "\n",
    "print(\"Fine-tuned BioBERT model saved at './fine_tuned_biobert_v3'\")\n",
    "print(\"Model is on:\", next(model.parameters()).device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix for outcomes_clinicalbiobert_predictions_test.csv:\n",
      "[[ 4002  5206]\n",
      " [11148 44039]]\n",
      "\n",
      "Classification Report for outcomes_clinicalbiobert_predictions_test.csv:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.26      0.43      0.33      9208\n",
      "     Class 1       0.89      0.80      0.84     55187\n",
      "\n",
      "    accuracy                           0.75     64395\n",
      "   macro avg       0.58      0.62      0.59     64395\n",
      "weighted avg       0.80      0.75      0.77     64395\n",
      "\n",
      "Predictions saved to outcomes_clinicalbiobert_predictions_test.csv\n",
      "\n",
      "Confusion Matrix for outcomes_clinicalbiobert_predictions_train.csv:\n",
      "[[ 36012    322]\n",
      " [ 24289 196954]]\n",
      "\n",
      "Classification Report for outcomes_clinicalbiobert_predictions_train.csv:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.60      0.99      0.75     36334\n",
      "     Class 1       1.00      0.89      0.94    221243\n",
      "\n",
      "    accuracy                           0.90    257577\n",
      "   macro avg       0.80      0.94      0.84    257577\n",
      "weighted avg       0.94      0.90      0.91    257577\n",
      "\n",
      "Predictions saved to outcomes_clinicalbiobert_predictions_train.csv\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on testing data (test_df) using \"Outcomes\" column\n",
    "evaluate_model(\n",
    "    dataframe=test_df,\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    device=device,\n",
    "    output_csv_name=folder_path + \"outcomes_clinicalbiobert_predictions_test.csv\",\n",
    "    text_column=\"Outcomes\",\n",
    "    max_length=128  # Specific max_length for \"Outcomes\"\n",
    ")\n",
    "\n",
    "# Evaluate on training data (df) using \"Outcomes\" column\n",
    "evaluate_model(\n",
    "    dataframe=df,\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    device=device,\n",
    "    output_csv_name=folder_path + \"outcomes_clinicalbiobert_predictions_train.csv\",\n",
    "    text_column=\"Outcomes\",\n",
    "    max_length=128  # Specific max_length for \"Outcomes\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NEST",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
